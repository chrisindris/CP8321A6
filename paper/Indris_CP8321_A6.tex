\documentclass[conference]{IEEEtran}

\usepackage{cite}
\usepackage{amsmath,amssymb,amsfonts}
\usepackage{algorithmic}
\usepackage{graphicx}
\usepackage{textcomp}
\usepackage{xcolor}

\usepackage{pgfplots} % plotting
\pgfplotsset{compat = newest}

\usepackage[numbered,framed]{matlab-prettifier}
\usepackage{filecontents}
\usepackage{comment}
\usepackage{verbatimbox}
\usepackage{adjustbox}

\usepackage{nicematrix,tikz}
\usetikzlibrary{tikzmark}

% macros
\newcommand{\dd}[2]{\frac{\partial #1}{\partial #2}} % partial derivative

% path
\graphicspath{{/Users/lorybuttazzoni/Documents/TMU/CP8307-CV/Assignments/A2/code/output/}}

\def\BibTeX{{\rm B\kern-.05em{\sc i\kern-.025em b}\kern-.08em
    T\kern-.1667em\lower.7ex\hbox{E}\kern-.125emX}}
\begin{document}

\title{CP8321 - Assignment 3\\
}

\author{\IEEEauthorblockN{Christopher Indris}
\IEEEauthorblockA{\textit{Department of Computer Science} \\
\textit{Toronto Metropolitan University}\\
Toronto, Canada \\
christopher.indris@ryerson.ca}}

\maketitle

%%% START HERE

%%% Abstract

\begin{abstract}
We would like to express the derivative of total error $E$ with respect to an output $y_{j}$, when neuron $j$ is an output neuron and also when neuron $j$ is a hidden neuron. We can also use sigmoid as a specific activation function. 
\end{abstract}

%%% Case 1
\section{Part A, Case 1: Output Layer}

The objective in this section is to calculate $\dd{E_{c}}{y_{j, c}}$. In this case, we assume that neuron $j$ is an output neuron--- it is in the output layer of the neural network. So, $j$ will index over neurons in the output layer. \\

If $j$ is an output neuron, $y_{j}$ is an output of the neural network which feeds directly into $E_{c}$, the error for a particular case $c$. \\

$\dd{E_{c}}{y_{j,c}}$ \\

$= \dd{}{y_{j,c}} \frac{1}{2} \sum_{c',j'}(y_{j',c'} - d_{j',c'})^{2}$ \\

$= \dd{}{y_{j,c}} \frac{1}{2} \sum_{c',j' \neq c,j}(y_{j',c'} - d_{j',c'})^{2} + \dd{}{y_{j,c}} \frac{1}{2}(y_{j,c} - d_{j,c})^{2}$ \\

$= \dd{}{y_{j,c}} \frac{1}{2}(y_{j,c} - d_{j,c})^{2}$ \\

$= y_{j,c} - d_{j,c}$ \\

In addition, we can compute the error signal $\delta_{j, c} = \dd{E_{c}}{x_{j,c}}$ using what we have calculated above. To calculate $\delta_{j,c}$, we will apply the chain rule. We also recognize that the activation function $f$ can be expressed as $y_{j, c} = f(x_{j, c})$, where $x_{j, c}$ is the total input to node $j$ for case $c$. \\

$\delta_{j, c}$ \\

$= \dd{E_{c}}{x_{j,c}}$ \\

$= \dd{E_{c}}{y_{j,c}} \cdot \dd{y_{j,c}}{x_{j,c}}$ \\

$= (y_{j,c} - d_{j,c}) \cdot f'(x_{j,c})$ \\


%%% Case 2
\section{Part B, Case 2: Hidden Layer}

The objective in this section is to calculate $\dd{E_{c}}{y_{j, c}}$ again, but neuron $j$ is in a (general) hidden layer. Since $j$ can no longer be used to index over the output layer, we can use $l$ to index over the output layer instead. For brevity, we can set $e_{l, c} = d_{l, c} - y_{l, c}$, and so we can write a version of the error function as follows:

\begin{equation}
E_{c} = \frac{1}{2}\sum_{l}e_{l, c}^{2}
\end{equation}

We can express our objective using the chain rule as follows:

\begin{equation}
\dd{E_{c}}{y_{j, c}} = \dd{E_{c}}{e_{j, c}} \cdot \dd{e_{l, c}}{x_{l, c}} \cdot \dd{x_{l, c}}{y_{j, c}}
\end{equation}

For the first derivative in the chain, we can calculate it using the chain rule:

\begin{equation}
\dd{E_{c}}{e_{j, c}} = \dd{}{e_{j, c}} \frac{1}{2}\sum_{l}e_{l, c}^{2} = \sum_{l}e_{i, c} = \sum_{l}(d_{l,c} - y_{l,c})
\end{equation}

For the second derivative in the chain (notice that $e_{l, c}$ is the sum of a constant and an activation function):

\begin{equation}
\dd{e_{l, c}}{x_{l, c}} = \dd{}{x_{l,c}}(d_{l,c} - y_{l, c}) = -f'(x_{l,c})
\end{equation}

Putting together what we have: \\

$\dd{E_{c}}{y_{j, c}} = \sum_{l}(y_{l, c} - d_{j, c}) \cdot f'(x_{l, c}) \cdot \dd{x_{l, c}}{y_{j, c}} = \sum_{l} \delta_{l, c} \dd{x_{l, c}}{y_{j, c}}$ \\

The first part looks like the error signal from the previous section. Lastly, we must calculate the final derivative in the chain, which connects the total input to the output layer ($x_{l, c}$) to the output of node $j$ in the hidden layer. \\

$\dd{x_{l, c}}{y_{j, c}}$ \\

$= \dd{x_{l, c}}{y_{k, c}} \cdot \dd{y_{k, c}}{y_{k-1, c}} \cdot \ldots \cdot \dd{y_{j+1, c}}{y_{j, c}}$ \\

$= w_{lk,c} \cdot \dd{y_{k, c}}{y_{k-1, c}} \cdot \ldots \cdot \dd{y_{j+1, c}}{y_{j, c}}$ \\

Lots of repetition is present as the path makes its way back to the layer with node $j$, with term $\dd{y_{k,c}}{y_{k-1,c}}$ looking like $\sum_{k}f'(x_{k,c})w_{k,k+1,c}$ and so on. However, we can use the $\delta$ to avoid a very long equation. \\

In Case 1, where $j$ was in the output layer, we had $\delta_{j, c} = \dd{E_{c}}{x_{j, c}}$. We can define this for any $j$, in accordance with equation $\delta_{j}^{k} = \dd{E}{a_{j}^{k}} = \sum_{l=1}^{r^{k+1}} \dd{E}{a_{l}^{k+1}} \dd{a_{l}^{k+1}}{a_{j}^{k}}$ \cite{b1}. \\

$\delta_{j}$ \\

$= \dd{E_{c}}{x_{j, c}}$ \\

Applying the chain rule, and using $k$ to index over the nodes in the next layer, we attain: \\

$= \sum_{k} \dd{E_{c}}{x_{k, c}} \cdot \dd{x_{k, c}}{x_{j, c}}$ \\

If we are generalizing $\delta$, we can use $\delta_{k, c} = \dd{E_{c}}{x_{k, c}}$ and define $\delta$ recursively: \\

$= \sum_{k} \delta_{k} \cdot \dd{x_{k, c}}{x_{j, c}}$ \\

And, we replace the second term: \\

$= \sum_{k} \delta_{k} \cdot \dd{x_{k, c}}{y_{j, c}} \cdot \dd{y_{j,c}}{x_{j,c}}$ \\

$= \sum_{k} \delta_{k} \cdot w_{kj, c} \cdot f'(x_{j, c})$ \\

Now $\delta$ has been generalized and it can be used to simplify Equation \verb+(2)+. We would like to find a path along the computation graph from $E_{c}$ to $y_{j,c}$; the general $\delta$ can take us all the way to the total input to a node (some $x$), so using the chain rule we will get as close as we can to $y_{j,c}$ (which is $x_{k,c}$, the input to the layer in front of $j$) and then take the additional small step from $x_{k, c}$ to $y_{j, c}$. This is summarized in Equation \verb+(5)+:

\begin{equation}
\dd{E_{c}}{y_{j,c}} = \dd{E_{c}}{x_{k,c}} \cdot \dd{x_{k,c}}{y_{j,c}}
\end{equation}

Total inputs to a node are weighted sums, so:

\begin{equation}
\dd{x_{k,c}}{y_{j,c}} = \dd{}{y_{j,c}} \sum_{j}w_{kj,c}y_{j,c} = w_{kj,c}
\end{equation}

We can now input $\delta$ to complete Equation \verb+(5)+, noting that (like Equation 4 of backpropagation) we must sum over all of the nodes in the next layer since they will be receiving $y_{j,c}$ as input:

\begin{equation}
\dd{x_{k,c}}{y_{j,c}} = \sum_{k}\delta_{k,c}w_{kj,c}
\end{equation}

%%% Case 3
\section{Part C: Sigmoid Activation Function}

In this section, we will specify our general activation function $f$ and assume that $f = \sigma$, the sigmoid function. The definition is included here:

\begin{equation}
y_{j} = \sigma(x_{j}) = \frac{1}{1 + e^{-x_{j}}}
\end{equation}

Also, the derivative: \\

$\dd{y_{j}}{x_{j}}$ \\

$= \dd{}{x_{j}} \sigma(x_{j})$ \\

$= \dd{}{x_{j}} \frac{1}{1 + e^{-x_{j}}}$ \\

$= \frac{e^{-x_{j}}}{(1 + e^{-x_{j}})^{2}}$ \\

$= \frac{1}{1 + e^{-x_{j}}} \cdot \frac{e^{-x_{j}}}{1 + e^{-x_{j}}}$ \\

$= y_{j} \cdot \frac{e^{-x_{j}}}{1 + e^{-x_{j}}}$ \\

$= y_{j} \cdot \frac{(1 - 1) + e^{-x_{j}}}{1 + e^{-x_{j}}}$ \\

$= y_{j} \cdot \big[ \frac{1 + e^{-x_{j}}}{1 + e^{-x_{j}}} - \frac{1}{1 + e^{-x_{j}}} \big]$ \\

$= y_{j}(1 - y_{j})$ \\

For this step, we can simply replace all instances of $f$ and $f'$ according to $f(x_{j}) = y_{j} = \sigma(x_{j})$ and $f'(x_{j}) = y_{j}(1 - y_{j}) = \sigma(x_{j})(1 - \sigma(x_{j}))$. Equation \verb+(2)+ would be re-written as the following, in accordance with Equations \verb+(3)+ and \verb+(4)+:

\begin{equation}
\dd{E_{c}}{y_{j, c}} = \sum_{l}(d_{j,c} - y_{l,c})y_{l,c}(1 - y_{l,c})\dd{x_{l,c}}{y_{j,c}}
\end{equation}

The general $\delta_{j}$ becomes:

\begin{equation}
\delta_{j} = \sum_{k}\delta_{k} \cdot w_{kj,c} \cdot y_{j}(1 - y_{j})
\end{equation}

%%% Bibliography

\begin{thebibliography}{00}

\bibitem{b1} J. McGonagle, G. Shaikouski, C. Williams, A. Hsu, J. Khim, and A. Miller, “Backpropagation,” Brilliant Math \& Science Wiki. [Online]. Available: https://brilliant.org/wiki/backpropagation/. [Accessed: 18-Oct-2022]. 

\end{thebibliography}
\vspace{12pt}

\end{document}
